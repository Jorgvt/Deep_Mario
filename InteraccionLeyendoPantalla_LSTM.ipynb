{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('Master': conda)",
   "metadata": {
    "interpreter": {
     "hash": "57d46a1f3f975f92cc34d815bf69a7d3644582cc16f1cedc66cb95f17202c91e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import numpy as np\n",
    "import time\n",
    "import pydirectinput\n",
    "import cv2\n",
    "import mss\n",
    "import time\n",
    "import pywinauto\n",
    "import win32gui\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from KeyPress import PressKey, ReleaseKey\n",
    "from collections import deque"
   ]
  },
  {
   "source": [
    "# Cargamos el modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_dict = {\"up\": \"x\", \"down\": \"b\", \"left\": \"y\", \"right\": \"a\", \"none\": \"\"}\n",
    "keys_dict_di = {\"up\": 0x2D, \"down\": 0x30, \"left\": 0x15, \"right\": 0x1E, \"none\": 0x2D}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['down', 'none', 'right']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "classes = ['down', 'none', 'right']\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero hay que definir el modelo\n",
    "class TemporalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TemporalModel, self).__init__()\n",
    "        self.convolutions = nn.Sequential(*[\n",
    "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding = 1, stride = 1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding = 1, stride = 1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(4),\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding = 1, stride = 1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(4),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64*14*18, 100),\n",
    "        nn.ReLU(),\n",
    "    ])\n",
    "        self.hidden_size = 50\n",
    "        self.lstm = nn.LSTM(input_size = 100, hidden_size = self.hidden_size, num_layers = 1, batch_first = True, dropout=0.5)\n",
    "        #Definimos el estado oculto\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_size).to(device),\n",
    "                            torch.zeros(1,1,self.hidden_size).to(device))\n",
    "        \n",
    "        self.final_linear = nn.Linear(self.hidden_size, 3)\n",
    "\n",
    "    def forward(self, x):      \n",
    "        pred = torch.empty(x.shape[0],3,100).to(device)\n",
    "\n",
    "        for i in range(x.shape[1]):\n",
    "            pred[:,i,:] = self.convolutions(x[:,i,:,:,:])\n",
    "\n",
    "        pred, self.hidden_cell = self.lstm(pred, self.hidden_cell)\n",
    "\n",
    "\n",
    "        #Tenemos que hacerle detach al hidden_cell porque eso no se ajusta\n",
    "        self.hidden_cell = tuple(a.detach() for a in self.hidden_cell)\n",
    "\n",
    "        # Nos quedamos solamente con el elemento que corresponde al último\n",
    "        # instante temporal\n",
    "        pred = self.final_linear(pred[:,-1,:])\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\vila_\\anaconda3\\envs\\Master\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TemporalModel(\n",
       "  (convolutions): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Flatten(start_dim=1, end_dim=-1)\n",
       "    (9): Linear(in_features=16128, out_features=100, bias=True)\n",
       "    (10): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(100, 50, batch_first=True, dropout=0.5)\n",
       "  (final_linear): Linear(in_features=50, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model_custom = TemporalModel()\n",
    "#Cargamos los pesos del modelo entrenado\n",
    "model_custom.load_state_dict(torch.load(\"ModeloLSTM_11_12_13_522_BotonRojo.pth\"))\n",
    "model_custom.eval()\n",
    "model_custom.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(img):\n",
    "    \"\"\"\n",
    "    Transformaciones necesarias para poder meter las imágenes en la red\n",
    "\n",
    "    Argumentos:\n",
    "    img -> Imagen\n",
    "\n",
    "    Returns:\n",
    "    img -> Imagen transformada\n",
    "    \"\"\"\n",
    "    #La pasamos a RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGB)\n",
    "    #Recortamos solo la sección que nos interesa\n",
    "    img = img[49:-12,20:-20]\n",
    "    #Ajustamos la imagen al tamaño que espera la red\n",
    "    img = cv2.resize(img, dsize = (293,224))\n",
    "    #Transformamos la imagen a Tensor\n",
    "    img = torchvision.transforms.ToTensor()(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deque_to_tensor(deque):\n",
    "    \"\"\"\n",
    "    Cogemos el buffer de imágenes (deque) y lo convertimos en un tensor con la forma adecuada.\n",
    "\n",
    "    Argumentos:\n",
    "    deque -> deque de imágenes (buffer)\n",
    "\n",
    "    Returns:\n",
    "    tensor_ready -> torch.Tensor listo para entrar a la red LSTM\n",
    "    \"\"\"\n",
    "    tensor_ready = torch.empty(1,3, 3, 224, 293)\n",
    "    for i in range(len(img_buffer)):\n",
    "        tensor_ready[:,i,:] = img_buffer[i]\n",
    "    return tensor_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(model, data, classes):\n",
    "    \"\"\"\n",
    "    Predecimos la clase correspondiente para una imagen dada con el modelo deseado.\n",
    "    El preprocesado de la imagen se hace dentro de la función, no fuera.\n",
    "\n",
    "    Args:\n",
    "    model -> Modelo que queremos usar para hacer la predicción\n",
    "    data -> Conjunto de imágenes que usamos para predecir\n",
    "    classes -> Lista de las posibles clases\n",
    "\n",
    "    Returns:\n",
    "    Acción en formato str\n",
    "    \"\"\"\n",
    "\n",
    "    data = data.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(data)\n",
    "    return classes[torch.softmax(pred, dim = 1).argmax(dim = 1)]"
   ]
  },
  {
   "source": [
    "# Interacción con la pantalla"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(956, 269, 1499, 776)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "#Left, Top, Right, Bottom\n",
    "window_coords = win32gui.GetWindowRect(pywinauto.findwindows.find_window(title_re = \"Super Mario\"))\n",
    "window_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "left, top, right, bottom = window_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = right - left\n",
    "height = bottom - top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# font \n",
    "font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "  \n",
    "# org \n",
    "org = (50, 50) \n",
    "  \n",
    "# fontScale \n",
    "fontScale = 1\n",
    "   \n",
    "# Blue color in BGR \n",
    "color = (255, 0, 0) \n",
    "  \n",
    "# Line thickness of 2 px \n",
    "thickness = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_dict_di = {\"up\": 0x2D, \"down\": 0x30, \"left\": 0x15, \"right\": 0x1E, \"none\": 0x2D}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time.sleep(2)\n",
    "\n",
    "img_buffer = deque(maxlen=3)\n",
    "\n",
    "with mss.mss() as  sct:\n",
    "    monitor = {\"top\": top, \"left\": left, \"width\": width, \"height\": height}\n",
    "\n",
    "    while \"Screen Recording\": #Aquí da igual lo que pongas porque una string es como si fuera un True\n",
    "        last_time = time.time()\n",
    "\n",
    "        #Esto coge las imágenes en BGRA\n",
    "        sct_img = sct.grab(monitor)\n",
    "        img = np.array(sct_img)\n",
    "        img_transformed = transform_image(img)\n",
    "        img_buffer.append(img_transformed)\n",
    "        \n",
    "        if len(img_buffer) < 3:\n",
    "            continue\n",
    "        #Transformamos el deque a un tensor\n",
    "        tensor_ready = deque_to_tensor(img_buffer)\n",
    "\n",
    "        #Predecimos la acción con nuestro modelo\n",
    "        action = predict_action(model_custom, tensor_ready, classes)\n",
    "\n",
    "        # pydirectinput.press(keys_dict[action], _pause = False)\n",
    "\n",
    "        text = \"Action: {} fps: {}\".format(action, 1 / (time.time() - last_time))\n",
    "        tries = 0\n",
    "        if action != \"none\":\n",
    "            tries = 4\n",
    "            for i in range(tries):\n",
    "                PressKey(keys_dict_di[action])\n",
    "                time.sleep(0.001)\n",
    "                ReleaseKey(keys_dict_di[action])\n",
    "\n",
    "        img = cv2.putText(img, text, org, font,  \n",
    "                   fontScale, color, thickness, cv2.LINE_AA) \n",
    "\n",
    "        cv2.imshow(\"Cuac\", img)\n",
    "\n",
    "        #Press \"q\" to quit\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  }
 ]
}